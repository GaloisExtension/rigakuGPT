# -*- coding: utf-8 -*-
import streamlit as st
import os
import tempfile
import shutil
from PIL import Image
import subprocess
import openai
import google.generativeai as genai
import base64
import cv2
import numpy as np
import io
from dotenv import load_dotenv

# èªè¨¼ãƒ»èª²é‡‘ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from auth import (
    init_auth_session, login_required, show_login_page, 
    handle_oauth_callback, show_user_info, check_usage_limit, 
    increment_usage, sync_subscription_status, logout
)
from payment import (
    show_pricing_page, handle_payment_callback, 
    init_payment_session, verify_premium_access,
    manage_subscription
)

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# Gemini APIè¨­å®š
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="RigakuGPT",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .main > div {
        padding-top: 2rem;
    }
    .stApp > header {
        background-color: transparent;
    }
    .css-18e3th9 {
        padding-top: 1rem;
        padding-bottom: 1rem;
    }
    h1 {
        color: #1f77b4;
        font-family: 'Arial', sans-serif;
        text-align: center;
        margin-bottom: 2rem;
    }
    h2 {
        color: #2c3e50;
        border-bottom: 2px solid #3498db;
        padding-bottom: 0.5rem;
    }
    .stButton > button {
        width: 100%;
        border-radius: 20px;
        border: none;
        padding: 0.5rem 1rem;
        font-weight: 600;
        transition: all 0.3s;
    }
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 8px rgba(0,0,0,0.2);
    }
</style>
""", unsafe_allow_html=True)

def main():
    # èªè¨¼ãƒ»èª²é‡‘ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–
    init_auth_session()
    init_payment_session()
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
    if 'latex_code' not in st.session_state:
        st.session_state.latex_code = ""
    if 'pdf_path' not in st.session_state:
        st.session_state.pdf_path = None
    if 'ai_response' not in st.session_state:
        st.session_state.ai_response = ""
    
    # OAuthãƒ»æ”¯æ‰•ã„ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
    handle_oauth_callback()
    handle_payment_callback()
    
    # ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯
    if not st.session_state.get("authenticated"):
        show_login_page()
        return

    # ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ã®å ´åˆã€Stripeã®ã‚µãƒ–ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’åŒæœŸ
    user_id = st.session_state.user_info.get("sub")
    user_email = st.session_state.user_info.get("email")
    if user_id and user_email:
        sync_subscription_status(user_id, user_email)
    
    # ã‚¿ãƒ–è¨­å®š
    tab1, tab2 = st.tabs(["ğŸ’¬ ãƒ¡ã‚¤ãƒ³", "ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±"])
    
    with tab1:
        # ãƒ˜ãƒƒãƒ€ãƒ¼
        st.markdown("""
        <div style="text-align: center; padding: 2rem 0;">
            <h1 style="color: white;">RigakuGPT</h1>
        </div>
        """, unsafe_allow_html=True)
        
        # èªè¨¼ãƒã‚§ãƒƒã‚¯
        if not st.session_state.get('authenticated', False):
            show_login_page()
            return
        
        # ãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’ãƒ¡ã‚¤ãƒ³ç”»é¢ä¸Šéƒ¨ã«è¡¨ç¤º
        show_plan_info_main()
        
        # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        show_main_content()
    
    with tab2:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ãƒ»è¨­å®šã‚¿ãƒ–
        show_user_tab()
    
def show_plan_info_main():
    """ãƒ¡ã‚¤ãƒ³ç”»é¢ä¸Šéƒ¨ã«ãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’è¡¨ç¤º"""
    if st.session_state.get('authenticated', False):
        user_info = st.session_state.user_info
        plan = st.session_state.user_plan
        
        # ãƒ—ãƒ©ãƒ³è¡¨ç¤º
        plan_color = "#28a745" if plan == 'premium' else "#6c757d"
        plan_text = "Premium" if plan == 'premium' else "Free"
        
        # ä½¿ç”¨çŠ¶æ³ã®å–å¾—
        if plan == 'free':
            ocr_count = st.session_state.get('ocr_usage_count', 0)
            question_count = st.session_state.get('question_usage_count', 0)
            usage_text = f"OCR: {ocr_count}/20å› | è³ªå•: {question_count}/20å› | çŠ¶æ…‹ã¯1æ—¥ã§ãƒªã‚»ãƒƒãƒˆã•ã‚Œã¾ã™ã€‚" 
        else:
            usage_text = "ç„¡åˆ¶é™åˆ©ç”¨å¯èƒ½"
        
        # ãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã«è¡¨ç¤º
        st.markdown(f"""
        <div style="background-color: #f8f9fa; border-radius: 10px; padding: 1rem; margin-bottom: 1.5rem; border-left: 4px solid {plan_color};">
            <div style="display: flex; justify-content: space-between; align-items: center;">
                <div>
                    <span style="font-weight: bold; color: {plan_color};">{plan_text}ãƒ—ãƒ©ãƒ³</span>
                    <span style="margin-left: 1rem; color: #6c757d; font-size: 0.9rem;">{usage_text}</span>
                </div>
                <div style="color: #6c757d; font-size: 0.9rem;">
                    {user_info['email']}
                </div>
            </div>
        </div>
        """, unsafe_allow_html=True)

def show_main_content():
    """ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¡¨ç¤º"""
    # API ã‚­ãƒ¼ã®è¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
    if not os.environ.get("OPENAI_API_KEY"):
        st.error("OpenAI APIã®ã‚¨ãƒ©ãƒ¼ã€‚ç®¡ç†è€…@ClashRoyale_Hã¾ã§ã”é€£çµ¡ãŠé¡˜ã„ã—ã¾ã™ã€‚")
        st.stop()
    
    if not os.environ.get("GEMINI_API_KEY"):
        st.error("Gemini APIã®ã‚¨ãƒ©ãƒ¼ã€‚ç®¡ç†è€…@ClashRoyale_Hã¾ã§ã”é€£çµ¡ãŠé¡˜ã„ã—ã¾ã™ã€‚")
        st.stop()
    
    # è¿½åŠ ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’åˆæœŸåŒ–
    if 'additional_response' not in st.session_state:
        st.session_state.additional_response = ""
    if 'response_pdf_path' not in st.session_state:
        st.session_state.response_pdf_path = None
    if 'last_question' not in st.session_state:
        st.session_state.last_question = ""
    
    # è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆãƒšãƒ¼ã‚¸ä¸Šéƒ¨ï¼‰
    st.markdown("### ğŸ¤– ç”»åƒèªè­˜")
    st.info("ç”»åƒã‹ã‚‰æ•°å¼ã‚’èªè­˜ã—ã¾ã™")
    
    st.markdown("### ğŸ“¤ ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒªã‚¢ï¼ˆè¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼‰
    uploaded_files = st.file_uploader(
        "ğŸ“ ç”»åƒã‚’é¸æŠï¼ˆè¤‡æ•°å¯ï¼‰",
        type=['png', 'jpg', 'jpeg'],
        accept_multiple_files=True,
        help="è³ªå•ã—ãŸã„å†…å®¹ã‚’å«ã‚€ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚"
    )
    enable_preprocessing = st.checkbox(
        "ç”»åƒå‰å‡¦ç†ã‚’æœ‰åŠ¹åŒ–",
        value=True,
        help="ç”»åƒã‚’å‡¦ç†ã—ã€è¦‹ã‚„ã™ãã—ã¾ã™ã€‚"
    )
    
    if uploaded_files:
        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã‚’è¡¨ç¤º
        st.markdown(f"**ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ•°:** {len(uploaded_files)}æš")
        
        # å‰å‡¦ç†ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ã®ã‚³ãƒ³ãƒ†ãƒŠ
        processed_images = []
        
        if enable_preprocessing:
            with st.spinner("ğŸ”§ ç”»åƒå‰å‡¦ç†ä¸­..."):
                for uploaded_file in uploaded_files:
                    processed_img, success = preprocess_image(uploaded_file)
                    processed_images.append(processed_img)
                    if not success:
                        st.warning(f"{uploaded_file.name} ã®å‰å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        # ç”»åƒè¡¨ç¤ºï¼ˆå‰å‡¦ç†ãŒæœ‰åŠ¹ãªå ´åˆã¯å‰å‡¦ç†æ¸ˆã¿ã®ã¿ã€ç„¡åŠ¹ãªå ´åˆã¯å…ƒç”»åƒã®ã¿ï¼‰
        if enable_preprocessing and processed_images:
            # å‰å‡¦ç†æ¸ˆã¿ç”»åƒã®ã¿è¡¨ç¤º
            if len(uploaded_files) == 1:
                st.image(processed_images[0], use_column_width=True)
            else:
                cols = st.columns(min(len(uploaded_files), 3))
                for i, (uploaded_file, processed_img) in enumerate(zip(uploaded_files, processed_images)):
                    with cols[i % 3]:
                        st.image(processed_img, use_column_width=True)
        else:
            # å‰å‡¦ç†ãªã—ã®å ´åˆã¯å…ƒç”»åƒã®ã¿è¡¨ç¤º
            if len(uploaded_files) == 1:
                image = Image.open(uploaded_files[0])
                st.image(image, use_column_width=True)
            else:
                cols = st.columns(min(len(uploaded_files), 3))
                for i, uploaded_file in enumerate(uploaded_files):
                    with cols[i % 3]:
                        image = Image.open(uploaded_file)
                        st.image(image, use_column_width=True)
        
        # OCR å®Ÿè¡Œãƒœã‚¿ãƒ³
        if st.button("ğŸ” ã“ã®æ–‡ç« ã‚’èª­ã¿è¾¼ã‚€", type="primary"):
            # ä½¿ç”¨åˆ¶é™ãƒã‚§ãƒƒã‚¯
            if not check_usage_limit('ocr'):
                return
            
            with st.spinner(f"{len(uploaded_files)}æšã®ç”»åƒã‚’èª­ã¿å–ã‚Šä¸­..."):
                # ãƒ—ãƒ©ãƒ³ã«å¿œã˜ã¦OCRãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
                if st.session_state.user_plan == 'premium':
                    ocr_model = "gpt-4o"  # Premiumãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«
                else:
                    ocr_model = "gpt-4o-mini"  # Freeãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯æ¨™æº–ãƒ¢ãƒ‡ãƒ«

                # å‰å‡¦ç†ãŒæœ‰åŠ¹ãªå ´åˆã¯å‰å‡¦ç†æ¸ˆã¿ç”»åƒã‚’ä½¿ç”¨
                if enable_preprocessing and processed_images:
                    latex_result = perform_ocr_with_processed_images(processed_images, uploaded_files, model=ocr_model)
                else:
                    latex_result = perform_ocr_with_multiple_images(uploaded_files, model=ocr_model)
                    
                if latex_result:
                    st.session_state.latex_code = latex_result
                    # ä½¿ç”¨å›æ•°ã‚’ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆ
                    increment_usage('ocr')
                    st.success("âœ… èª­ã¿å–ã‚Šå®Œäº†!")
                    
                    # èªè­˜çµæœã‚’è¡¨ç¤ºï¼ˆç”Ÿã®TeX + ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°æ¸ˆã¿ï¼‰
                    st.markdown("### ğŸ“„ èªè­˜çµæœ")
                    render_latex_content(latex_result)
                    
                else:
                    st.error("âŒ èª­ã¿å–ã‚Šã«å¤±æ•—ã—ã¾ã—ãŸ")
            
    else:
        # ç”»åƒãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„å ´åˆã®ã¿è¡¨ç¤º
        st.info("è³ªå•ã—ãŸã„éƒ¨åˆ†ã®ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼ˆè¤‡æ•°æšå¯ï¼‰")
        
        # æ‰‹å‹•å…¥åŠ›ãƒœã‚¿ãƒ³
        if st.button("ğŸ“ TeXã‚’æ‰‹å‹•ã§å…¥åŠ›"):
            st.session_state.latex_code = ""
            st.info("TeXæ‰‹å‹•å…¥åŠ›ã—ã€è³ªå•ãŒã§ãã¾ã™")
    
    # ãƒ†ã‚¹ãƒˆç”¨ã‚¯ã‚¤ãƒƒã‚¯å…¥åŠ›ï¼ˆç”»åƒãŒãªã„å ´åˆã®ã¿è¡¨ç¤ºï¼‰
    if not uploaded_files:
        with st.expander("ğŸ§ª ãƒ†ã‚¹ãƒˆç”¨ã®æ•°å¼ã§è©¦ã™"):
            col_test1, col_test2 = st.columns([1, 1])
            with col_test1:
                if st.button("äºŒæ¬¡æ–¹ç¨‹å¼ã«ã¤ã„ã¦çŸ¥ã‚ŠãŸã„"):
                    st.session_state.latex_code = "äºŒæ¬¡æ–¹ç¨‹å¼ã®è§£ã®å…¬å¼\n\n$$x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$$\n\nã“ã“ã§ã€$a$, $b$, $c$ ã¯ä¿‚æ•°ã§ã‚ã‚‹ã€‚"
            with col_test2:
                if st.button("ç©åˆ†ã«ã¤ã„ã¦çŸ¥ã‚ŠãŸã„"):
                    st.session_state.latex_code = "å®šç©åˆ†ã®åŸºæœ¬å®šç†\n\n$$\\int_a^b f(x) dx = F(b) - F(a)$$\n\nãŸã ã—ã€$F(x)$ ã¯ $f(x)$ ã®åŸå§‹é–¢æ•°ã§ã‚ã‚‹ã€‚"

    st.markdown("### ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆç·¨é›† & PDFç”Ÿæˆ")
    
    latex_code = st.text_area(
        "ğŸ“„ èªè­˜ã«èª¤ã‚ŠãŒã‚ã‚‹å ´åˆã¯ã“ã“ã§ç·¨é›†ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™",
        value=st.session_state.get('latex_code', ''),
        height=250,
        help="èªè­˜ã•ã‚ŒãŸå†…å®¹ã‚’ç¢ºèªãƒ»ç·¨é›†ã—ã¦ãã ã•ã„",
        placeholder="ã“ã“ã«èªè­˜ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒ»æ•°å¼ãŒè¡¨ç¤ºã•ã‚Œã¾ã™..."
    )
    
    if latex_code != st.session_state.get('latex_code', ''):
        st.session_state.latex_code = latex_code
    
    # PDF ç”Ÿæˆãƒœã‚¿ãƒ³
    if st.button("ğŸ“„ å…¥åŠ›ã‚’PDFã§ç¢ºèªã™ã‚‹", disabled=not latex_code):
        with st.spinner("ğŸ“„ PDFç”Ÿæˆä¸­..."):
            pdf_path = generate_pdf(latex_code)
            if pdf_path:
                st.session_state.pdf_path = pdf_path
                st.success("âœ… PDFç”Ÿæˆå®Œäº†!")
            else:
                st.error("âŒ PDFç”Ÿæˆå¤±æ•—")
    
    # PDF ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º
    pdf_path = st.session_state.get('pdf_path')
    if pdf_path and os.path.exists(pdf_path):
        st.header("ğŸ“„ PDF")
        
        # PDF ã‚’ Base64 ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦è¡¨ç¤º
        with open(pdf_path, "rb") as f:
            base64_pdf = base64.b64encode(f.read()).decode('utf-8')
        
        pdf_display = f'<iframe src="data:application/pdf;base64,{base64_pdf}" width="100%" height="500" type="application/pdf"></iframe>'
        st.markdown(pdf_display, unsafe_allow_html=True)

    # --- ã‚·ãƒ³ãƒ—ãƒ«ã§å®‰å®šã—ãŸãƒãƒ£ãƒƒãƒˆUI ---
    st.markdown("---")
    st.markdown("### ğŸ’¬ ãƒãƒ£ãƒƒãƒˆ")
    
    # ãƒ¢ãƒ‡ãƒ«ã®è¡¨ç¤ºå
    MODEL_DISPLAY_NAMES = {
        "gemini-1.5-flash-latest": "âš¡ é«˜é€Ÿãªå¿œç­”ã«æœ€é©",
        "gpt-4o-mini": "ğŸ¤– ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸæ€§èƒ½",
        "gemini-1.5-pro-latest": "ğŸ§  é«˜åº¦ãªæ¨è«–ã«æœ€é©"
    }
    
    # ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆãƒ—ãƒ¬ãƒŸã‚¢ãƒ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã¿ï¼‰
    chat_model = "gemini-1.5-flash-latest"  # ç„¡æ–™ãƒ—ãƒ©ãƒ³ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    if st.session_state.user_plan == 'premium':
        chat_model = st.selectbox(
            "ğŸ¤– ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
            options=list(MODEL_DISPLAY_NAMES.keys()),
            format_func=lambda model_id: MODEL_DISPLAY_NAMES[model_id],
            help="Premium: é«˜æ€§èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã§ãã¾ã™",
            index=0
        )
    else:
        st.info(f"ğŸ”¥{MODEL_DISPLAY_NAMES['gemini-1.5-flash-latest']}ãªæ¨è«–ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒãƒ£ãƒƒãƒˆã—ã¾ã™ã€‚")

    # ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®åˆæœŸåŒ–
    if "chat_messages" not in st.session_state:
        st.session_state.chat_messages = []

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
    for message in st.session_state.chat_messages:
        with st.chat_message(message["role"]):
            render_latex_content(message["content"])

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›ã‚’å—ã‘å–ã‚‹
    if prompt := st.chat_input("æ•°å¼ã«ã¤ã„ã¦è³ªå•ãƒ»è¿½åŠ è³ªå•ã—ã¦ãã ã•ã„..."):
        # latex_codeãŒãªã‘ã‚Œã°ä½•ã‚‚ã—ãªã„
        if not st.session_state.get('latex_code'):
            st.warning("ã¾ãšã€ä¸Šã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§æ•°å¼ã‚’å«ã‚€ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¾ãŸã¯ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            st.stop()
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ  & è¡¨ç¤º
        st.session_state.chat_messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            render_latex_content(prompt)

        # ä½¿ç”¨åˆ¶é™ãƒã‚§ãƒƒã‚¯ & AIå¿œç­”
        if not check_usage_limit('question'):
             # åˆ¶é™è¶…éãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒãƒ£ãƒƒãƒˆã«è¿½åŠ  & è¡¨ç¤º
            limit_msg = "ç„¡æ–™ãƒ—ãƒ©ãƒ³ã®è³ªå•å›æ•°ä¸Šé™ã«é”ã—ã¾ã—ãŸã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚¿ãƒ–ã‹ã‚‰Premiumãƒ—ãƒ©ãƒ³ã¸ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’ã”æ¤œè¨ãã ã•ã„ã€‚"
            st.session_state.chat_messages.append({"role": "assistant", "content": limit_msg})
            with st.chat_message("assistant"):
                render_latex_content(limit_msg)
        else:
            # AIã‹ã‚‰ã®å¿œç­”ã‚’ç”Ÿæˆ & è¡¨ç¤º
            with st.chat_message("assistant"):
                with st.spinner("ğŸ¤– AIãŒå›ç­”ã‚’ç”Ÿæˆä¸­..."):
                    context = build_chat_context(st.session_state.chat_messages, st.session_state.latex_code)
                    
                    # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ã¦å¿œç­”ã‚’ç”Ÿæˆ
                    if chat_model.startswith("gemini"):
                        response = get_gemini_response(context, chat_model)
                    else:  # gpt-4o-mini
                        response = get_ai_response_simple(context)
                
                    if response:
                        render_latex_content(response)
                        # å¿œç­”ã‚’å±¥æ­´ã«è¿½åŠ 
                        st.session_state.chat_messages.append({"role": "assistant", "content": response})
                        increment_usage('question')
                    else:
                        error_msg = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
                        render_latex_content(error_msg)
                        st.session_state.chat_messages.append({"role": "assistant", "content": error_msg})

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ãŒã‚ã‚‹å ´åˆã®è£œåŠ©ãƒœã‚¿ãƒ³
    if st.session_state.get('chat_messages'):
        st.markdown("---")
        
        if st.button("ğŸ—‘ï¸ ãƒãƒ£ãƒƒãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ", use_container_width=True):
            st.session_state.chat_messages = []
            st.rerun()
        
        if st.button("ğŸ“„ ä¼šè©±ã‚’PDFã§å‡ºåŠ›", use_container_width=True):
            with st.spinner("ğŸ“„ PDFç”Ÿæˆä¸­..."):
                pdf_path = generate_conversation_pdf(st.session_state.chat_messages, st.session_state.get('latex_code', ''))
                if pdf_path:
                    st.session_state.response_pdf_path = pdf_path
                    st.success("âœ… PDFç”Ÿæˆå®Œäº†!")
                else:
                    st.error("âŒ PDFç”Ÿæˆå¤±æ•—")
        
        if st.session_state.get('response_pdf_path'):
            with open(st.session_state.response_pdf_path, "rb") as pdf_file:
                st.download_button(
                    label="ğŸ’¾ PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=pdf_file.read(),
                    file_name="rigakugpt_conversation.pdf",
                    mime="application/pdf",
                    use_container_width=True
                )

def show_user_tab():
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ãƒ»è¨­å®šã‚¿ãƒ–ã‚’è¡¨ç¤º"""
    st.markdown("# ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±")
    
    if st.session_state.get('authenticated', False):
        user_info = st.session_state.user_info
        plan = st.session_state.user_plan
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±è¡¨ç¤ºï¼ˆåå‰ã¯é™¤å¤–ï¼‰
        col1, col2 = st.columns([1, 3])
        
        with col1:
            if user_info.get('picture'):
                st.image(user_info['picture'], width=100)
        
        with col2:
            st.markdown(f"**Email:** {user_info['email']}")
            
            # ãƒ—ãƒ©ãƒ³è¡¨ç¤º
            plan_color = "#28a745" if plan == 'premium' else "#6c757d"
            plan_text = "Premium" if plan == 'premium' else "Free"
            st.markdown(f"**ãƒ—ãƒ©ãƒ³:** <span style='color: {plan_color}; font-weight: bold;'>{plan_text}</span>", 
                       unsafe_allow_html=True)
        
        # ä½¿ç”¨åˆ¶é™è¡¨ç¤º
        if plan == 'free':
            # ä½¿ç”¨å›æ•°ã‚’å–å¾—
            ocr_count = st.session_state.get('ocr_usage_count', 0)
            question_count = st.session_state.get('question_usage_count', 0)
            
            st.markdown("---")
            st.markdown("### ğŸ“Š ä½¿ç”¨çŠ¶æ³")
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("OCRå®Ÿè¡Œ", f"{ocr_count}å›")
            with col2:
                st.metric("è³ªå•å›æ•°", f"{question_count}å›")
        else:
            st.markdown("---")
            st.markdown("### â­ Premium")
            st.success("ç„¡åˆ¶é™åˆ©ç”¨å¯èƒ½")
        
        # æ–™é‡‘ãƒ—ãƒ©ãƒ³è¡¨ç¤º
        st.markdown("---")
        show_pricing_page()
        
        # ãƒ­ã‚°ã‚¢ã‚¦ãƒˆãƒœã‚¿ãƒ³
        st.markdown("---")
        if st.button("ğŸšª ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ", use_container_width=True, type="secondary"):
            logout()
    else:
        st.warning("ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ã§ã™")

def preprocess_image(image_file):
    """
    æ•™ç§‘æ›¸ç”»åƒã®å‰å‡¦ç†ï¼šã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·åŒ– + å½©åº¦å‰Šé™¤
    """
    try:
        # PILã‹ã‚‰OpenCVå½¢å¼ã«å¤‰æ›
        pil_image = Image.open(image_file)
        
        # RGBã«å¤‰æ›ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
        if pil_image.mode != 'RGB':
            pil_image = pil_image.convert('RGB')
        
        # numpyé…åˆ—ã«å¤‰æ›
        image_array = np.array(pil_image)
        
        # BGRã‹ã‚‰RGBã«å¤‰æ›ï¼ˆOpenCVç”¨ï¼‰
        image_bgr = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)
        
        # 1. å½©åº¦ã‚’å‰Šé™¤ï¼ˆã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«åŒ–ï¼‰
        gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)
        
        # 2. ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·åŒ–ï¼ˆã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ç”»åƒã«å¯¾ã—ã¦CLAHEé©ç”¨ï¼‰
        clahe = cv2.createCLAHE(clipLimit=1.3, tileGridSize=(8, 8))
        enhanced_gray = clahe.apply(gray)
        
        # 3. ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ç”»åƒã‚’RGBã«æˆ»ã™ï¼ˆ3ãƒãƒ£ãƒ³ãƒãƒ«ï¼‰
        final_bgr = cv2.cvtColor(enhanced_gray, cv2.COLOR_GRAY2BGR)
        final_rgb = cv2.cvtColor(final_bgr, cv2.COLOR_BGR2RGB)
        
        # PIL Imageã«å¤‰æ›
        processed_image = Image.fromarray(final_rgb)
        
        return processed_image, True
        
    except Exception as e:
        st.warning(f"ç”»åƒå‰å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å…ƒã®ç”»åƒã‚’è¿”ã™
        return Image.open(image_file), False

def render_latex_content(text):
    """LaTeXæ··åœ¨ãƒ†ã‚­ã‚¹ãƒˆã‚’Streamlitã§ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰"""
    try:
        # ç©ºæ–‡å­—ãƒã‚§ãƒƒã‚¯
        if not text or not text.strip():
            st.warning("èªè­˜ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™")
            return
        
        # st.markdownã¯LaTeXï¼ˆKaTeXï¼‰ã‚’ç›´æ¥ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ãŸã‚ã€
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãã®ã¾ã¾æ¸¡ã™ã ã‘ã§è‰¯ã„
        st.markdown(text, unsafe_allow_html=True)
                    
    except Exception as e:
        st.error(f"ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {str(e)}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã§ã‚‚ã€å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¥µåŠ›è¡¨ç¤ºã—ã‚ˆã†ã¨è©¦ã¿ã‚‹
        st.markdown("**ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¡¨ç¤º:**")
        st.text(text)

def perform_ocr_with_processed_images(processed_images, original_files, model="gpt-4o-mini"):
    """å‰å‡¦ç†æ¸ˆã¿ç”»åƒã‹ã‚‰GPT Visionã‚’ä½¿ç”¨ã—ã¦å…¨ã¦ã®æ–‡å­—ãƒ»æ•°å¼ã‚’æŠ½å‡º"""
    try:
        # è¤‡æ•°ç”»åƒã®base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        image_contents = []
        
        for i, (processed_img, original_file) in enumerate(zip(processed_images, original_files)):
            # PIL Imageã‚’BytesIOã«å¤‰æ›
            img_buffer = io.BytesIO()
            processed_img.save(img_buffer, format='PNG')
            img_buffer.seek(0)
            
            # base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            image_base64 = base64.b64encode(img_buffer.getvalue()).decode('utf-8')
            
            image_contents.append({
                "type": "image_url", 
                "image_url": {
                    "url": f"data:image/png;base64,{image_base64}",
                    "detail": "high"
                }
            })
        
        # OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
        client = openai.OpenAI(
            api_key=os.environ.get("OPENAI_API_KEY"),
            default_headers={}  # ã‚«ã‚¹ã‚¿ãƒ ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚¯ãƒªã‚¢
        )
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰ï¼ˆå‰å‡¦ç†æ¸ˆã¿ç”»åƒå¯¾å¿œï¼‰
        content = [
            {
                "type": "text",
                "text": f"ã“ã‚Œã‚‰{len(processed_images)}æšã®å‰å‡¦ç†æ¸ˆã¿ç”»åƒã«å«ã¾ã‚Œã‚‹å…¨ã¦ã®æ–‡å­—ãƒ»æ•°å¼ã‚’æ­£ç¢ºã«èª­ã¿å–ã£ã¦ã€æ­£ç¢ºã«æ›¸ãå‡ºã—ã¦ãã ã•ã„ã€‚æ•°å¼ã¯LaTeXè¨˜æ³•ã§è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã‚‰ã®ç”»åƒã¯èª­ã¿å–ã‚Šã‚„ã™ãã™ã‚‹ãŸã‚ã«å‰å‡¦ç†ï¼ˆã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·åŒ–ã€ãƒã‚¤ã‚ºé™¤å»ç­‰ï¼‰ãŒæ–½ã•ã‚Œã¦ã„ã¾ã™ã€‚è¤‡æ•°ã®ç”»åƒãŒã‚ã‚‹å ´åˆã¯ã€é †ç•ªã«å†…å®¹ã‚’çµ±åˆã—ã¦1ã¤ã®æ–‡æ›¸ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
            }
        ]
        content.extend(image_contents)
        
        response = client.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "system",
                    "content": """ã‚ãªãŸã¯é«˜ç²¾åº¦ãªOCRã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚å‰å‡¦ç†æ¸ˆã¿ã®ç”»åƒã‹ã‚‰å…¨ã¦ã®æ–‡å­—ãƒ»æ•°å¼ã‚’æ­£ç¢ºã«èª­ã¿å–ã£ã¦ãã ã•ã„ã€‚

                ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ãã ã•ã„ï¼š
                1. å‰å‡¦ç†ã«ã‚ˆã‚Šã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆãŒå¼·åŒ–ã•ã‚Œã€æ–‡å­—ãŒé®®æ˜ã«ãªã£ã¦ã„ã¾ã™
                2. æ•°å¼ã¯é©åˆ‡ãªLaTeXè¨˜æ³•ã§è¡¨ç¾ã™ã‚‹ï¼ˆ\\frac, \\sum, \\int, \\sqrt ãªã©ï¼‰
                3. é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ãã®ã¾ã¾è¨˜è¿°ã™ã‚‹
                4. ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆæ®µè½ã€æ”¹è¡Œï¼‰ã‚’å¯èƒ½ãªé™ã‚Šä¿æŒã™ã‚‹
                5. æ•°å¼ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’é©åˆ‡ã«åŒºåˆ¥ã™ã‚‹
                6. ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³æ•°å¼ã¯ $ $ ã§ã€ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤æ•°å¼ã¯ $$ $$ ã§å›²ã‚€
                7. å‰å‡¦ç†ã«ã‚ˆã‚Šæ–‡å­—å¢ƒç•ŒãŒå¼·èª¿ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€ç´°ã‹ã„è¨˜å·ã‚„ä¸Šä»˜ããƒ»ä¸‹ä»˜ãæ–‡å­—ã‚‚æ­£ç¢ºã«èª­ã¿å–ã‚‹

                å‡ºåŠ›ä¾‹ï¼š
                ã€Œä¸‰è§’é–¢æ•°ã®å…¬å¼
                $\\sin^2 x + \\cos^2 x = 1$
                ã“ã‚Œã¯ä¸‰è§’é–¢æ•°ã®æœ€ã‚‚åŸºæœ¬çš„ãªæ’ç­‰å¼ã§ã‚ã‚‹ã€‚

                å¾®åˆ†ã®å®šç¾©
                $$\\frac{d}{dx} f(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$$ã€"""
                },
                {
                    "role": "user",
                    "content": content
                }
            ],
            max_tokens=3000  # è¤‡æ•°ç”»åƒãªã®ã§ä¸Šé™ã‚’å¢—ã‚„ã™
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        error_msg = str(e).encode('utf-8', errors='ignore').decode('utf-8')
        st.error(f"å‰å‡¦ç†æ¸ˆã¿ç”»åƒ OCR ã‚¨ãƒ©ãƒ¼: {error_msg}")
        st.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {type(e).__name__}")
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
        st.write("API ã‚­ãƒ¼è¨­å®šçŠ¶æ³:", "è¨­å®šæ¸ˆã¿" if os.environ.get("OPENAI_API_KEY") else "æœªè¨­å®š")
        st.write("ãƒ•ã‚¡ã‚¤ãƒ«æ•°:", len(uploaded_files) if uploaded_files else 0)
        return None

def perform_ocr_with_gemini_processed_images(processed_images, original_files):
    """å‰å‡¦ç†æ¸ˆã¿ç”»åƒã‹ã‚‰Gemini 1.5 Flashã‚’ä½¿ç”¨ã—ã¦å…¨ã¦ã®æ–‡å­—ãƒ»æ•°å¼ã‚’æŠ½å‡º"""
    try:
        # Gemini 1.5 Flash ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
        model = genai.GenerativeModel('gemini-1.5-flash-latest')
        
        # å‰å‡¦ç†æ¸ˆã¿ç”»åƒã‚’Geminiç”¨ã«æº–å‚™
        gemini_images = []
        for processed_img in processed_images:
            # PIL Imageã‚’BytesIOã«å¤‰æ›
            img_buffer = io.BytesIO()
            processed_img.save(img_buffer, format='PNG')
            img_buffer.seek(0)
            
            # Geminiç”¨ã®ç”»åƒã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
            gemini_images.append({
                'mime_type': 'image/png',
                'data': img_buffer.getvalue()
            })
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
        prompt = f"""ã“ã‚Œã‚‰{len(processed_images)}æšã®å‰å‡¦ç†æ¸ˆã¿ç”»åƒã«å«ã¾ã‚Œã‚‹å…¨ã¦ã®æ–‡å­—ãƒ»æ•°å¼ã‚’æ­£ç¢ºã«èª­ã¿å–ã£ã¦ã€æ­£ç¢ºã«æ›¸ãå‡ºã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ãã ã•ã„ï¼š
1. å‰å‡¦ç†ã«ã‚ˆã‚Šã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆãŒå¼·åŒ–ã•ã‚Œã€æ–‡å­—ãŒé®®æ˜ã«ãªã£ã¦ã„ã¾ã™
2. æ•°å¼ã¯é©åˆ‡ãªLaTeXè¨˜æ³•ã§è¡¨ç¾ã™ã‚‹ï¼ˆ\\frac, \\sum, \\int, \\sqrt ãªã©ï¼‰
3. é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ãã®ã¾ã¾è¨˜è¿°ã™ã‚‹
4. ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆæ®µè½ã€æ”¹è¡Œï¼‰ã‚’å¯èƒ½ãªé™ã‚Šä¿æŒã™ã‚‹
5. æ•°å¼ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’é©åˆ‡ã«åŒºåˆ¥ã™ã‚‹
6. ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³æ•°å¼ã¯ $ $ ã§ã€ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤æ•°å¼ã¯ $$ $$ ã§å›²ã‚€
7. è¤‡æ•°ã®ç”»åƒãŒã‚ã‚‹å ´åˆã¯ã€é †ç•ªã«å†…å®¹ã‚’çµ±åˆã—ã¦1ã¤ã®æ–‡æ›¸ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„

å‡ºåŠ›ä¾‹ï¼š
ã€Œä¸‰è§’é–¢æ•°ã®å…¬å¼
$\\sin^2 x + \\cos^2 x = 1$
ã“ã‚Œã¯ä¸‰è§’é–¢æ•°ã®æœ€ã‚‚åŸºæœ¬çš„ãªæ’ç­‰å¼ã§ã‚ã‚‹ã€‚

å¾®åˆ†ã®å®šç¾©
$$\\frac{{d}}{{dx}} f(x) = \\lim_{{h \\to 0}} \\frac{{f(x+h) - f(x)}}{{h}}$$ã€
"""
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å†…å®¹ã‚’æ§‹ç¯‰
        request_parts = [prompt]
        for img in gemini_images:
            request_parts.append(img)
        
        # Gemini APIã‚’å‘¼ã³å‡ºã—
        response = model.generate_content(request_parts)
        
        return response.text.strip()
        
    except Exception as e:
        error_msg = str(e).encode('utf-8', errors='ignore').decode('utf-8')
        st.error(f"Gemini OCR ã‚¨ãƒ©ãƒ¼: {error_msg}")
        st.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {type(e).__name__}")
        return None

def perform_ocr_with_gemini_multiple_images(uploaded_files):
    """è¤‡æ•°ã®ç”»åƒã‹ã‚‰Gemini 1.5 Flashã‚’ä½¿ç”¨ã—ã¦å…¨ã¦ã®æ–‡å­—ãƒ»æ•°å¼ã‚’æŠ½å‡º"""
    try:
        # Gemini 1.5 Flash ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
        model = genai.GenerativeModel('gemini-1.5-flash-latest')
        
        # ç”»åƒã‚’Geminiç”¨ã«æº–å‚™
        gemini_images = []
        for uploaded_file in uploaded_files:
            # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’åˆ¤å®š
            file_type = uploaded_file.type
            if file_type == "image/jpeg":
                mime_type = "image/jpeg"
            elif file_type == "image/png":
                mime_type = "image/png"
            elif file_type == "image/webp":
                mime_type = "image/webp"
            else:
                mime_type = "image/png"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            
            gemini_images.append({
                'mime_type': mime_type,
                'data': uploaded_file.getvalue()
            })
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
        prompt = f"""ã“ã‚Œã‚‰{len(uploaded_files)}æšã®ç”»åƒã«å«ã¾ã‚Œã‚‹å…¨ã¦ã®æ–‡å­—ãƒ»æ•°å¼ã‚’æ­£ç¢ºã«èª­ã¿å–ã£ã¦ã€æ­£ç¢ºã«æ›¸ãå‡ºã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ãã ã•ã„ï¼š
1. ç”»åƒã®å…¨ã¦ã®æ–‡å­—ã‚’æ¼ã‚ŒãªãæŠ½å‡ºã™ã‚‹
2. æ•°å¼ã¯é©åˆ‡ãªLaTeXè¨˜æ³•ã§è¡¨ç¾ã™ã‚‹ï¼ˆ\\frac, \\sum, \\int, \\sqrt ãªã©ï¼‰
3. é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ãã®ã¾ã¾è¨˜è¿°ã™ã‚‹
4. ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆæ®µè½ã€æ”¹è¡Œï¼‰ã‚’å¯èƒ½ãªé™ã‚Šä¿æŒã™ã‚‹
5. æ•°å¼ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’é©åˆ‡ã«åŒºåˆ¥ã™ã‚‹
6. ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³æ•°å¼ã¯ $ $ ã§ã€ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤æ•°å¼ã¯ $$ $$ ã§å›²ã‚€
7. è¤‡æ•°ã®ç”»åƒãŒã‚ã‚‹å ´åˆã¯ã€é †ç•ªã«å†…å®¹ã‚’çµ±åˆã—ã¦1ã¤ã®æ–‡æ›¸ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„

å‡ºåŠ›ä¾‹ï¼š
ã€Œä¸‰è§’é–¢æ•°ã®å…¬å¼
$\\sin^2 x + \\cos^2 x = 1$
ã“ã‚Œã¯ä¸‰è§’é–¢æ•°ã®æœ€ã‚‚åŸºæœ¬çš„ãªæ’ç­‰å¼ã§ã‚ã‚‹ã€‚

å¾®åˆ†ã®å®šç¾©
$$\\frac{{d}}{{dx}} f(x) = \\lim_{{h \\to 0}} \\frac{{f(x+h) - f(x)}}{{h}}$$ã€
"""
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å†…å®¹ã‚’æ§‹ç¯‰
        request_parts = [prompt]
        for img in gemini_images:
            request_parts.append(img)
        
        # Gemini APIã‚’å‘¼ã³å‡ºã—
        response = model.generate_content(request_parts)
        
        return response.text.strip()
        
    except Exception as e:
        error_msg = str(e).encode('utf-8', errors='ignore').decode('utf-8')
        st.error(f"Gemini OCR ã‚¨ãƒ©ãƒ¼: {error_msg}")
        st.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {type(e).__name__}")
        return None

def perform_ocr_with_multiple_images(uploaded_files, model="gpt-4o-mini"):
    """è¤‡æ•°ã®ç”»åƒã‹ã‚‰GPT Visionã‚’ä½¿ç”¨ã—ã¦å…¨ã¦ã®æ–‡å­—ãƒ»æ•°å¼ã‚’æŠ½å‡º"""
    try:
        # è¤‡æ•°ç”»åƒã®base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        image_contents = []
        
        for i, uploaded_file in enumerate(uploaded_files):
            image_base64 = base64.b64encode(uploaded_file.getvalue()).decode('utf-8')
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’åˆ¤å®š
            file_type = uploaded_file.type
            if file_type == "image/jpeg":
                mime_type = "image/jpeg"
            elif file_type == "image/png":
                mime_type = "image/png"
            elif file_type == "image/webp":
                mime_type = "image/webp"
            else:
                mime_type = "image/png"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            
            image_contents.append({
                "type": "image_url", 
                "image_url": {
                    "url": f"data:{mime_type};base64,{image_base64}",
                    "detail": "high"
                }
            })
        
        # OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
        client = openai.OpenAI(
            api_key=os.environ.get("OPENAI_API_KEY"),
            default_headers={}  # ã‚«ã‚¹ã‚¿ãƒ ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚¯ãƒªã‚¢
        )
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰ï¼ˆè¤‡æ•°ç”»åƒå¯¾å¿œï¼‰
        content = [
            {
                "type": "text",
                "text": f"ã“ã‚Œã‚‰{len(uploaded_files)}æšã®å‰å‡¦ç†æ¸ˆã¿ç”»åƒã«å«ã¾ã‚Œã‚‹å…¨ã¦ã®æ–‡å­—ãƒ»æ•°å¼ã‚’æ­£ç¢ºã«èª­ã¿å–ã£ã¦ã€æ­£ç¢ºã«æ›¸ãå‡ºã—ã¦ãã ã•ã„ã€‚æ•°å¼ã¯LaTeXè¨˜æ³•ã§è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã‚‰ã®ç”»åƒã¯èª­ã¿å–ã‚Šã‚„ã™ãã™ã‚‹ãŸã‚ã«å‰å‡¦ç†ï¼ˆã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·åŒ–ã€ãƒã‚¤ã‚ºé™¤å»ç­‰ï¼‰ãŒæ–½ã•ã‚Œã¦ã„ã¾ã™ã€‚è¤‡æ•°ã®ç”»åƒãŒã‚ã‚‹å ´åˆã¯ã€é †ç•ªã«å†…å®¹ã‚’çµ±åˆã—ã¦1ã¤ã®æ–‡æ›¸ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
            }
        ]
        content.extend(image_contents)
        
        response = client.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "system",
                    "content": """ã‚ãªãŸã¯é«˜ç²¾åº¦ãªOCRã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚è¤‡æ•°ã®ç”»åƒã«å«ã¾ã‚Œã‚‹å…¨ã¦ã®æ–‡å­—ãƒ»æ•°å¼ã‚’æ­£ç¢ºã«èª­ã¿å–ã£ã¦ãã ã•ã„ã€‚

                ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ãã ã•ã„ï¼š
                1. å‰å‡¦ç†ã«ã‚ˆã‚Šã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆãŒå¼·åŒ–ã•ã‚Œã€æ–‡å­—ãŒé®®æ˜ã«ãªã£ã¦ã„ã¾ã™
                2. æ•°å¼ã¯é©åˆ‡ãªLaTeXè¨˜æ³•ã§è¡¨ç¾ã™ã‚‹ï¼ˆ\\frac, \\sum, \\int, \\sqrt ãªã©ï¼‰
                3. é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ãã®ã¾ã¾è¨˜è¿°ã™ã‚‹
                4. ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆæ®µè½ã€æ”¹è¡Œï¼‰ã‚’å¯èƒ½ãªé™ã‚Šä¿æŒã™ã‚‹
                5. æ•°å¼ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’é©åˆ‡ã«åŒºåˆ¥ã™ã‚‹
                6. ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³æ•°å¼ã¯ $ $ ã§ã€ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤æ•°å¼ã¯ $$ $$ ã§å›²ã‚€
                7. å‰å‡¦ç†ã«ã‚ˆã‚Šæ–‡å­—å¢ƒç•ŒãŒå¼·èª¿ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€ç´°ã‹ã„è¨˜å·ã‚„ä¸Šä»˜ããƒ»ä¸‹ä»˜ãæ–‡å­—ã‚‚æ­£ç¢ºã«èª­ã¿å–ã‚‹

                å‡ºåŠ›ä¾‹ï¼š
                ã€Œä¸‰è§’é–¢æ•°ã®å…¬å¼
                $\\sin^2 x + \\cos^2 x = 1$
                ã“ã‚Œã¯ä¸‰è§’é–¢æ•°ã®æœ€ã‚‚åŸºæœ¬çš„ãªæ’ç­‰å¼ã§ã‚ã‚‹ã€‚

                å¾®åˆ†ã®å®šç¾©
                $$\\frac{d}{dx} f(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$$ã€"""
                },
                {
                    "role": "user",
                    "content": content
                }
            ],
            max_tokens=3000  # è¤‡æ•°ç”»åƒãªã®ã§ä¸Šé™ã‚’å¢—ã‚„ã™
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        error_msg = str(e).encode('utf-8', errors='ignore').decode('utf-8')
        st.error(f"GPT Vision OCR ã‚¨ãƒ©ãƒ¼: {error_msg}")
        st.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {type(e).__name__}")
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
        st.write("API ã‚­ãƒ¼è¨­å®šçŠ¶æ³:", "è¨­å®šæ¸ˆã¿" if os.environ.get("OPENAI_API_KEY") else "æœªè¨­å®š")
        st.write("ãƒ•ã‚¡ã‚¤ãƒ«æ•°:", len(uploaded_files) if uploaded_files else 0)
        return None

def perform_ocr_with_gpt(uploaded_file, model="gpt-4o-mini"):
    """GPT Vision ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‹ã‚‰å…¨ã¦ã®æ–‡å­—ãƒ»æ•°å¼ã‚’æŠ½å‡º"""
    try:
        # ç”»åƒã‚’base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        image_base64 = base64.b64encode(uploaded_file.getvalue()).decode('utf-8')
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’åˆ¤å®š
        file_type = uploaded_file.type
        if file_type == "image/jpeg":
            mime_type = "image/jpeg"
        elif file_type == "image/png":
            mime_type = "image/png"
        elif file_type == "image/webp":
            mime_type = "image/webp"
        else:
            mime_type = "image/png"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        
        # OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
        client = openai.OpenAI(
            api_key=os.environ.get("OPENAI_API_KEY"),
            default_headers={}  # ã‚«ã‚¹ã‚¿ãƒ ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚¯ãƒªã‚¢
        )
        
        response = client.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "system",
                    "content": """æšã®å‰å‡¦ç†æ¸ˆã¿ç”»åƒã«å«ã¾ã‚Œã‚‹å…¨ã¦ã®æ–‡å­—ãƒ»æ•°å¼ã‚’æ­£ç¢ºã«èª­ã¿å–ã£ã¦ã€æ­£ç¢ºã«æ›¸ãå‡ºã—ã¦ãã ã•ã„ã€‚æ•°å¼ã¯LaTeXè¨˜æ³•ã§è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã‚‰ã®ç”»åƒã¯èª­ã¿å–ã‚Šã‚„ã™ãã™ã‚‹ãŸã‚ã«å‰å‡¦ç†ï¼ˆã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·åŒ–ã€ãƒã‚¤ã‚ºé™¤å»ç­‰ï¼‰ãŒæ–½ã•ã‚Œã¦ã„ã¾ã™ã€‚

                ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ãã ã•ã„ï¼š
                1. ç”»åƒã®å…¨ã¦ã®æ–‡å­—ã‚’æ¼ã‚ŒãªãæŠ½å‡ºã™ã‚‹
                2. æ•°å¼ã¯é©åˆ‡ãªLaTeXè¨˜æ³•ã§è¡¨ç¾ã™ã‚‹ï¼ˆ\\frac, \\sum, \\int, \\sqrt ãªã©ï¼‰
                3. é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ãã®ã¾ã¾è¨˜è¿°ã™ã‚‹
                4. ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆæ®µè½ã€æ”¹è¡Œï¼‰ã‚’å¯èƒ½ãªé™ã‚Šä¿æŒã™ã‚‹
                5. æ•°å¼ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’é©åˆ‡ã«åŒºåˆ¥ã™ã‚‹
                6. ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³æ•°å¼ã¯ $ $ ã§ã€ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤æ•°å¼ã¯ $$ $$ ã§å›²ã‚€

                å‡ºåŠ›ä¾‹ï¼š
                ã€Œä¸‰è§’é–¢æ•°ã®å…¬å¼

                $\\sin^2 x + \\cos^2 x = 1$

                ã“ã‚Œã¯ä¸‰è§’é–¢æ•°ã®æœ€ã‚‚åŸºæœ¬çš„ãªæ’ç­‰å¼ã§ã‚ã‚‹ã€‚ã€"""
                },
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": "ã“ã®ç”»åƒã«å«ã¾ã‚Œã‚‹å…¨ã¦ã®æ–‡å­—ãƒ»æ•°å¼ã‚’æ­£ç¢ºã«èª­ã¿å–ã£ã¦ã€é©åˆ‡ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã¦ãã ã•ã„ã€‚æ•°å¼ã¯LaTeXè¨˜æ³•ã§è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚"
                        },
                        {
                            "type": "image_url", 
                            "image_url": {
                                "url": f"data:{mime_type};base64,{image_base64}",
                                "detail": "high"
                            }
                        }
                    ]
                }
            ],
            max_tokens=2000
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        error_msg = str(e).encode('utf-8', errors='ignore').decode('utf-8')
        st.error(f"GPT Vision OCR ã‚¨ãƒ©ãƒ¼: {error_msg}")
        st.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {type(e).__name__}")
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
        st.write("API ã‚­ãƒ¼è¨­å®šçŠ¶æ³:", "è¨­å®šæ¸ˆã¿" if os.environ.get("OPENAI_API_KEY") else "æœªè¨­å®š")
        st.write("ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—:", uploaded_file.type if hasattr(uploaded_file, 'type') else "ä¸æ˜")
        return None

def generate_pdf(latex_code):
    """LaTeX ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ PDF ã‚’ç”Ÿæˆ"""
    try:
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®‰å…¨åŒ–ï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰
        safe_latex_code = str(latex_code).encode('utf-8', errors='ignore').decode('utf-8')
        
        # LaTeX ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆuplatex + æ—¥æœ¬èªå¯¾å¿œï¼‰
        latex_document = f"""
\\documentclass[12pt,a4paper,uplatex]{{jsarticle}}
\\usepackage{{amsmath}}
\\usepackage{{amsfonts}}
\\usepackage{{amssymb}}
\\usepackage{{geometry}}
\\geometry{{margin=2cm}}

\\begin{{document}}

\\begin{{center}}
{{\\Large \\textbf{{rigakuGPT - æ•°å¼ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼}}}}
\\end{{center}}

\\vspace{{1cm}}

{safe_latex_code}

\\vfill
\\begin{{flushright}}
\\textit{{Generated by rigakuGPT}}
\\end{{flushright}}

\\end{{document}}
"""
        
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ PDF ç”Ÿæˆ
        with tempfile.TemporaryDirectory() as tmpdir:
            tex_path = os.path.join(tmpdir, "document.tex")
            
            # UTF-8ã§å®‰å…¨ã«æ›¸ãè¾¼ã¿
            try:
                with open(tex_path, 'w', encoding='utf-8', errors='ignore') as f:
                    f.write(latex_document)
            except UnicodeEncodeError:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ASCIIæ–‡å­—ã®ã¿ã§æ›¸ãè¾¼ã¿
                ascii_document = latex_document.encode('ascii', errors='ignore').decode('ascii')
                with open(tex_path, 'w', encoding='ascii') as f:
                    f.write(ascii_document)
            
            # uplatex + dvipdfmxã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼ˆæ‰‹å‹•å®Ÿè¡Œï¼‰
            # Step 1: uplatex ã§ .dvi ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
            try:
                result1 = subprocess.run([
                    'uplatex', 
                    '-interaction=nonstopmode',
                    '-halt-on-error',
                    'document.tex'
                ], cwd=tmpdir, capture_output=True, text=True, encoding='utf-8', errors='ignore')
                
                if result1.returncode != 0:
                    st.error("uplatex ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼:")
                    st.code(f"stdout: {result1.stdout}")
                    st.code(f"stderr: {result1.stderr}")
                    return None
                
                # Step 2: dvipdfmx ã§ .pdf ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
                result2 = subprocess.run([
                    'dvipdfmx', 
                    'document.dvi'
                ], cwd=tmpdir, capture_output=True, text=True, encoding='utf-8', errors='ignore')
                
                if result2.returncode != 0:
                    st.error("dvipdfmx ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼:")
                    st.code(f"stdout: {result2.stdout}")
                    st.code(f"stderr: {result2.stderr}")
                    return None
                
                pdf_path = os.path.join(tmpdir, "document.pdf")
                
                if os.path.exists(pdf_path):
                    # PDF ã‚’ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼
                    output_dir = "outputs"
                    os.makedirs(output_dir, exist_ok=True)
                    final_pdf_path = os.path.join(output_dir, "preview.pdf")
                    shutil.copy2(pdf_path, final_pdf_path)
                    return final_pdf_path
                else:
                    st.error("PDF ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                    return None
                    
            except FileNotFoundError as e:
                st.error(f"LaTeX ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {str(e)}")
                st.info("uplatex ã¨ dvipdfmx ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")
                return None
                
    except Exception as e:
        st.error(f"PDF ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

def build_conversation_context(chat_history, latex_code, new_question):
    """ä¼šè©±å±¥æ­´ã‚’å«ã‚€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰"""
    context = f"å‚è€ƒè³‡æ–™:\n{latex_code}\n\n"
    
    if chat_history:
        context += "ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´:\n"
        for i, (q, a) in enumerate(chat_history[-3:]):  # æœ€è¿‘ã®3ã¤ã®ä¼šè©±ã®ã¿ä½¿ç”¨
            context += f"è³ªå•{i+1}: {q}\nå›ç­”{i+1}: {a}\n\n"
    
    context += f"æ–°ã—ã„è³ªå•: {new_question}"
    return context

def build_conversation_context_from_messages(chat_messages, latex_code):
    """ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã‹ã‚‰ä¼šè©±å±¥æ­´ã‚’å«ã‚€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰"""
    context = f"å‚è€ƒè³‡æ–™:\n{latex_code}\n\n"
    
    if len(chat_messages) > 1:
        context += "ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´:\n"
        # æœ€æ–°ã®è³ªå•ã‚’é™¤ã„ãŸæœ€è¿‘ã®3çµ„ã®ä¼šè©±ã®ã¿ä½¿ç”¨
        previous_messages = chat_messages[:-1]  # æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é™¤ã
        recent_messages = previous_messages[-6:]  # æœ€å¤§6ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆ3çµ„ï¼‰
        
        conversation_pairs = []
        for i in range(0, len(recent_messages), 2):
            if i + 1 < len(recent_messages):
                user_msg = recent_messages[i]["content"]
                ai_msg = recent_messages[i + 1]["content"]
                conversation_pairs.append((user_msg, ai_msg))
        
        for i, (q, a) in enumerate(conversation_pairs):
            context += f"è³ªå•{i+1}: {q}\nå›ç­”{i+1}: {a}\n\n"
    
    # æœ€æ–°ã®è³ªå•ã‚’è¿½åŠ 
    if chat_messages:
        latest_question = chat_messages[-1]["content"]
        context += f"æ–°ã—ã„è³ªå•: {latest_question}"
    
    return context

def get_ai_response_with_context(conversation_context):
    """ä¼šè©±å±¥æ­´ã‚’è€ƒæ…®ã—ãŸAIå¿œç­”å–å¾—"""
    try:
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            st.error("OpenAI API ã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None
        
        client = openai.OpenAI(
            api_key=api_key,
            default_headers={}
        )
        
        response = client.chat.completions.create(
            model="o4-mini",
            messages=[
                {
                    "role": "system",
                    "content": """ã‚ãªãŸã¯ç§‘å­¦ã®å°‚é–€å®¶ã§ã™ã€‚ä¼šè©±å±¥æ­´ã‚’è¸ã¾ãˆã¦ã€ä¸€è²«æ€§ã®ã‚ã‚‹å›ç­”ã‚’ã—ã¦ãã ã•ã„ã€‚

å›ç­”ã™ã‚‹éš›ã®é‡è¦ãªãƒ«ãƒ¼ãƒ«ï¼š
1. ä¼šè©±å±¥æ­´ã‚’è€ƒæ…®ã—ã¦ã€å‰ã®è³ªå•ã¨ã®é–¢é€£æ€§ã‚’æ„è­˜ã™ã‚‹
2. ã€Œå…ˆã»ã©ã€ã€Œå‰å›ã€ãªã©ã®è¡¨ç¾ãŒã‚ã‚‹å ´åˆã¯ã€å±¥æ­´ã‚’å‚ç…§ã™ã‚‹
3. æ•°å¼ã¯å¿…ãšæ­£ç¢ºãªLaTeXè¨˜æ³•ã§è¡¨ç¾ã™ã‚‹
4. ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³æ•°å¼ã¯ $...$ ã§å›²ã‚€
5. ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤æ•°å¼ã¯ $$...$$ ã§å›²ã‚€
6. è¿½åŠ è³ªå•ã®å ´åˆã¯ã€å‰ã®å›ç­”ã‚’è¸ã¾ãˆã¦è£œè¶³èª¬æ˜ã™ã‚‹
7. ä¸€è²«ã—ãŸèª¬æ˜ã‚’å¿ƒãŒã‘ã€çŸ›ç›¾ã®ãªã„å›ç­”ã‚’ã™ã‚‹
8. å›ç­”ã¯ç°¡æ½”ã«ã¾ã¨ã‚ã€3-4æ®µè½ä»¥å†…ã«åã‚ã‚‹
9. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã«èª¤å­—è„±å­—ãŒã‚ã‚‹å ´åˆã¯ã€é©åˆ‡ã«è§£é‡ˆã—ã€å›ç­”ã‚’è¡Œã†

LaTeXè¨˜æ³•ã®ä¾‹ï¼š
- åˆ†æ•°: \\frac{åˆ†å­}{åˆ†æ¯}
- ä¸Šä»˜ã: x^{2}ã€ä¸‹ä»˜ã: x_{1}
- å¹³æ–¹æ ¹: \\sqrt{x}
- ç©åˆ†: \\int_{ä¸‹é™}^{ä¸Šé™} f(x) dx
- ç·å’Œ: \\sum_{i=1}^{n} a_i"""
                },
                {
                    "role": "user",
                    "content": conversation_context
                }
            ],
            max_tokens=600,
            temperature=0.7
        )
        
        result = response.choices[0].message.content
        return result
        
    except Exception as e:
        error_msg = str(e).encode('utf-8', errors='ignore').decode('utf-8')
        st.error(f"AI å¿œç­”ã‚¨ãƒ©ãƒ¼: {error_msg}")
        return None

def generate_conversation_pdf(chat_history, latex_code=""):
    """ä¼šè©±å±¥æ­´ã‚’PDFã¨ã—ã¦å‡ºåŠ›"""
    try:
        def safe_encode(text):
            if isinstance(text, bytes):
                text = text.decode('utf-8', errors='ignore')
            
            # LaTeXã®ç‰¹æ®Šæ–‡å­—ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
            # Note: ãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã¯æœ€åˆã«è¡Œã†å¿…è¦ãŒã‚ã‚‹
            text = str(text).replace('\\', '\\textbackslash{}')
            text = text.replace('{', '\\{')
            text = text.replace('}', '\\}')
            text = text.replace('&', '\\&')
            text = text.replace('%', '\\%')
            text = text.replace('$', '\\$')
            text = text.replace('#', '\\#')
            text = text.replace('_', '\\_')
            text = text.replace('^', '\\textasciicircum{}')
            text = text.replace('~', '\\textasciitilde{}')
            # è§’æ‹¬å¼§ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
            text = text.replace('[', '{[}')
            text = text.replace(']', '{]}')

            return str(text).encode('utf-8', errors='ignore').decode('utf-8')
        
        # ä¼šè©±å†…å®¹ã‚’æ§‹ç¯‰
        conversation_content = ""
        
        # chat_history (dictã®ãƒªã‚¹ãƒˆ) ã‚’ (è³ªå•, å›ç­”) ã®ãƒšã‚¢ã«å¤‰æ›
        qa_pairs = []
        # å¶æ•°ç•ªç›®ãŒuser, å¥‡æ•°ç•ªç›®ãŒassistantã§ã‚ã‚‹ã“ã¨ã‚’æœŸå¾…
        for i in range(0, len(chat_history) - 1, 2):
            if chat_history[i]['role'] == 'user' and chat_history[i+1]['role'] == 'assistant':
                qa_pairs.append((chat_history[i]['content'], chat_history[i+1]['content']))

        for i, (question, answer) in enumerate(qa_pairs):
            clean_question = clean_latex_for_pdf(safe_encode(question))
            clean_answer = clean_latex_for_pdf(safe_encode(answer))
            
            conversation_content += f"""
\\section*{{è³ªå• {i+1}}}
\\begin{{quote}}
{clean_question}
\\end{{quote}}

\\subsection*{{å›ç­”}}
{clean_answer}

\\vspace{{1cm}}
"""
        
        clean_latex_code = clean_latex_for_pdf(safe_encode(latex_code)) if latex_code else "ï¼ˆå‚ç…§å†…å®¹ãªã—ï¼‰"
        
        latex_document = f"""
\\documentclass[12pt,a4paper,uplatex]{{jsarticle}}
\\usepackage{{amsmath}}
\\usepackage{{amsfonts}}
\\usepackage{{amssymb}}
\\usepackage{{geometry}}
\\usepackage{{graphicx}}
\\usepackage{{hyperref}}
\\geometry{{margin=2.5cm}}

\\title{{\\textbf{{rigakuGPT ä¼šè©±ãƒ¬ãƒãƒ¼ãƒˆ}}}}
\\author{{\\textsc{{rigakuGPT}}}}
\\date{{\\today}}

\\begin{{document}}

\\maketitle

\\section*{{å‚è€ƒè³‡æ–™}}
{clean_latex_code}

{conversation_content}

\\vfill
\\begin{{center}}
\\textbf{{rigakuGPT}}
\\end{{center}}

\\end{{document}}
"""
        
        # PDFç”Ÿæˆå‡¦ç†
        with tempfile.TemporaryDirectory() as tmpdir:
            tex_path = os.path.join(tmpdir, "conversation.tex")
            
            try:
                with open(tex_path, 'w', encoding='utf-8', errors='ignore') as f:
                    f.write(latex_document)
            except UnicodeEncodeError:
                ascii_document = latex_document.encode('ascii', errors='ignore').decode('ascii')
                with open(tex_path, 'w', encoding='ascii') as f:
                    f.write(ascii_document)
            
            # uplatex + dvipdfmxã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
            try:
                # Step 1: uplatex ã§ .dvi ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
                result1 = subprocess.run([
                    'uplatex', 
                    '-interaction=nonstopmode',
                    '-halt-on-error',
                    'conversation.tex'
                ], cwd=tmpdir, capture_output=True, text=True, encoding='utf-8', errors='ignore')
                
                if result1.returncode != 0:
                    st.error("ä¼šè©±PDF uplatex ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼:")
                    st.text(f"stdout: {result1.stdout}")
                    st.text(f"stderr: {result1.stderr}")
                    return None
                
                # Step 2: dvipdfmx ã§ .pdf ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
                result2 = subprocess.run([
                    'dvipdfmx', 
                    'conversation.dvi'
                ], cwd=tmpdir, capture_output=True, text=True, encoding='utf-8', errors='ignore')
                
                if result2.returncode != 0:
                    st.error("ä¼šè©±PDF dvipdfmx ã‚¨ãƒ©ãƒ¼:")
                    st.text(f"stdout: {result2.stdout}")
                    st.text(f"stderr: {result2.stderr}")
                    return None
                
                pdf_path = os.path.join(tmpdir, "conversation.pdf")
                
                if os.path.exists(pdf_path):
                    # ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã‚“ã§è¿”ã™
                    with open(pdf_path, 'rb') as f:
                        pdf_data = f.read()
                    
                    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã•ã›ãšã«æ°¸ç¶šçš„ãªãƒ‘ã‚¹ã‚’è¿”ã™
                    final_pdf_path = os.path.join(tempfile.gettempdir(), f"conversation_{os.urandom(8).hex()}.pdf")
                    with open(final_pdf_path, 'wb') as f:
                        f.write(pdf_data)
                    return final_pdf_path
                else:
                    st.error("ç”Ÿæˆã•ã‚ŒãŸPDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                    return None

            except FileNotFoundError:
                st.error("uplatex/dvipdfmxãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚TeX LiveãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                return None
            except Exception as e:
                st.error(f"PDFç”Ÿæˆä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                return None

    except Exception as e:
        st.error(f"PDFç”Ÿæˆã®æº–å‚™ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def get_ai_response_simple(context):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªAIå¿œç­”å–å¾—ï¼ˆGPT-4o-miniãªã©ï¼‰"""
    try:
        client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": context['system_prompt']},
                {"role": "user", "content": context['user_input']}
            ],
            max_tokens=3000,
        )
        return response.choices[0].message.content
    except Exception as e:
        error_msg = str(e).encode('utf-8', errors='ignore').decode('utf-8')
        st.error(f"GPT å¿œç­”ã‚¨ãƒ©ãƒ¼: {error_msg}")
        return None

def get_gemini_response(context, model_name="gemini-1.5-flash-latest"):
    """Geminiãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦å¿œç­”ã‚’å–å¾—"""
    try:
        # ãƒ¢ãƒ‡ãƒ«è¨­å®š
        if model_name == "gemini-1.5-flash-latest":
            # ç„¡æ–™ãƒ—ãƒ©ãƒ³ã¯æ€è€ƒãƒ¢ãƒ¼ãƒ‰ï¼ˆä¸­ï¼‰ã‚’ä½¿ç”¨
            if st.session_state.user_plan == 'free':
                model = genai.GenerativeModel(
                    'gemini-1.5-flash-latest',
                    generation_config={
                    }
                )
            else:
                model = genai.GenerativeModel('gemini-1.5-flash-latest')
        elif model_name == "gemini-1.5-pro-latest":
            model = genai.GenerativeModel('gemini-1.5-pro-latest')
        else:
            model = genai.GenerativeModel('gemini-1.5-flash-latest')
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        system_prompt = """ã‚ãªãŸã¯ç§‘å­¦ã®å°‚é–€å®¶ã§ã™ã€‚ä¼šè©±å±¥æ­´ã‚’è¸ã¾ãˆã¦ã€ä¸€è²«æ€§ã®ã‚ã‚‹å›ç­”ã‚’ã—ã¦ãã ã•ã„ã€‚

å›ç­”ã™ã‚‹éš›ã®é‡è¦ãªãƒ«ãƒ¼ãƒ«ï¼š
1. ä¼šè©±å±¥æ­´ã‚’è€ƒæ…®ã—ã¦ã€å‰ã®è³ªå•ã¨ã®é–¢é€£æ€§ã‚’æ„è­˜ã™ã‚‹
2. ã€Œå…ˆã»ã©ã€ã€Œå‰å›ã€ãªã©ã®è¡¨ç¾ãŒã‚ã‚‹å ´åˆã¯ã€å±¥æ­´ã‚’å‚ç…§ã™ã‚‹
3. æ•°å¼ã¯å¿…ãšæ­£ç¢ºãªLaTeXè¨˜æ³•ã§è¡¨ç¾ã™ã‚‹
4. ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³æ•°å¼ã¯ $...$ ã§å›²ã‚€
5. ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤æ•°å¼ã¯ $$...$$ ã§å›²ã‚€
6. è¿½åŠ è³ªå•ã®å ´åˆã¯ã€å‰ã®å›ç­”ã‚’è¸ã¾ãˆã¦è£œè¶³èª¬æ˜ã™ã‚‹
7. ä¸€è²«ã—ãŸèª¬æ˜ã‚’å¿ƒãŒã‘ã€çŸ›ç›¾ã®ãªã„å›ç­”ã‚’ã™ã‚‹
8. å›ç­”ã¯ç°¡æ½”ã«ã¾ã¨ã‚ã€3-4æ®µè½ä»¥å†…ã«åã‚ã‚‹
9. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã«èª¤å­—è„±å­—ãŒã‚ã‚‹å ´åˆã¯ã€é©åˆ‡ã«è§£é‡ˆã—ã€å›ç­”ã‚’è¡Œã†9. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã«èª¤å­—è„±å­—ãŒã‚ã‚‹å ´åˆã¯ã€é©åˆ‡ã«è§£é‡ˆã—ã€å›ç­”ã‚’è¡Œã†

LaTeXè¨˜æ³•ã®ä¾‹ï¼š
- åˆ†æ•°: \\frac{åˆ†å­}{åˆ†æ¯}
- ä¸Šä»˜ã: x^{2}ã€ä¸‹ä»˜ã: x_{1}
- å¹³æ–¹æ ¹: \\sqrt{x}
- ç©åˆ†: \\int_{ä¸‹é™}^{ä¸Šé™} f(x) dx
- ç·å’Œ: \\sum_{i=1}^{n} a_i"""
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        prompt = f"{system_prompt}\n\n{context}"
        
        # Gemini APIã‚’å‘¼ã³å‡ºã—
        response = model.generate_content(prompt)
        
        return response.text.strip()
        
    except Exception as e:
        error_msg = str(e).encode('utf-8', errors='ignore').decode('utf-8')
        st.error(f"Gemini API ã‚¨ãƒ©ãƒ¼: {error_msg}")
        return None

def get_ai_response(latex_code, question):
    """GPT-4o mini ã«è³ªå•ã—ã¦å›ç­”ã‚’å–å¾—"""
    try:
        # OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
        client = openai.OpenAI(
            api_key=os.environ.get("OPENAI_API_KEY"),
            default_headers={}  # ã‚«ã‚¹ã‚¿ãƒ ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚¯ãƒªã‚¢
        )
        
        response = client.chat.completions.create(
            model="o4-mini",
            messages=[
                {
                    "role": "system",
                    "content": """ã‚ãªãŸã¯ç§‘å­¦ã®å°‚é–€å®¶ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰æä¾›ã•ã‚Œã‚‹æ•°å¼ãƒ»ãƒ†ã‚­ã‚¹ãƒˆã«ã¤ã„ã¦ã€ã‚ã‹ã‚Šã‚„ã™ãä¸å¯§ã«è§£èª¬ã—ã¦ãã ã•ã„ã€‚

å›ç­”ã™ã‚‹éš›ã®é‡è¦ãªãƒ«ãƒ¼ãƒ«ï¼š
1. æ•°å¼ã¯å¿…ãšæ­£ç¢ºãªLaTeXè¨˜æ³•ã§è¡¨ç¾ã™ã‚‹
2. ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³æ•°å¼ã¯ $...$ ã§å›²ã‚€
3. ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤æ•°å¼ã¯ $$...$$ ã§å›²ã‚€
4. LaTeXè¨˜æ³•ã®ä¾‹ï¼š
   - åˆ†æ•°: \\frac{åˆ†å­}{åˆ†æ¯}
   - ä¸Šä»˜ã: x^{2}ã€ä¸‹ä»˜ã: x_{1}
   - å¹³æ–¹æ ¹: \\sqrt{x}
   - ç©åˆ†: \\int_{ä¸‹é™}^{ä¸Šé™} f(x) dx
   - ç·å’Œ: \\sum_{i=1}^{n} a_i
   - ã‚®ãƒªã‚·ãƒ£æ–‡å­—: \\alpha, \\beta, \\gamma ãªã©
   - é–¢æ•°: \\sin, \\cos, \\log, \\ln ãªã©
5. æ•°å¼ã‚’å«ã‚€æ–‡ç« ã§ã¯ã€æ•°å¼éƒ¨åˆ†ã®ã¿LaTeXè¨˜æ³•ã‚’ä½¿ç”¨
6. æ•°å­¦çš„ã«æ­£ç¢ºã§åˆå­¦è€…ã«ã‚‚ã‚ã‹ã‚Šã‚„ã™ã„èª¬æ˜
7. å›ç­”ã¯ç°¡æ½”ã«ã¾ã¨ã‚ã€3-4æ®µè½ä»¥å†…ã«åã‚ã‚‹
8. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã«èª¤å­—è„±å­—ãŒã‚ã‚‹å ´åˆã¯ã€é©åˆ‡ã«è§£é‡ˆã—ã€å›ç­”ã‚’è¡Œã†

å‡ºåŠ›ä¾‹ï¼š
ã€Œã“ã®å¼ $f(x) = ax^2 + bx + c$ ã¯äºŒæ¬¡é–¢æ•°ã‚’è¡¨ã—ã¦ã„ã¾ã™ã€‚
è§£ã®å…¬å¼ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š
$$x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$$ã€"""
                },
                {
                    "role": "user",
                    "content": f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ»æ•°å¼ã«ã¤ã„ã¦è³ªå•ãŒã‚ã‚Šã¾ã™ï¼š

å†…å®¹:
{latex_code}

è³ªå•:
{question}

ã“ã®å†…å®¹ã«ã¤ã„ã¦è©³ã—ãæ•™ãˆã¦ãã ã•ã„ã€‚æ•°å¼ã¯æ­£ç¢ºãªLaTeXè¨˜æ³•ã§è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚"""
                }
            ],
            max_tokens=600,
            temperature=0.7
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        error_msg = str(e).encode('utf-8', errors='ignore').decode('utf-8')
        st.error(f"AI å¿œç­”ã‚¨ãƒ©ãƒ¼: {error_msg}")
        return None

def clean_latex_for_pdf(text):
    """AIã®å›ç­”ã‹ã‚‰LaTeXç”¨ã«æ–‡å­—åˆ—ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ã§å …ç‰¢ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼‰"""
    import re
    
    try:
        if isinstance(text, bytes):
            text = text.decode('utf-8', errors='ignore')
        elif isinstance(text, str):
            text = text.encode('utf-8', errors='ignore').decode('utf-8')
        
        # æ•°å¼ãƒ–ãƒ­ãƒƒã‚¯ã‚’ä¿è­· ($$...$$ ã¨ $...$)
        math_blocks = []
        def protect_math_block(match):
            placeholder = f"PLACEHOLDERMATHBLOCK{len(math_blocks)}"
            math_blocks.append(match.group(0))
            return placeholder
        
        text = re.sub(r'\$\$.*?\$\$', protect_math_block, text, flags=re.DOTALL)
        text = re.sub(r'\$[^$]*?\$', protect_math_block, text)

        # LaTeXã®ç‰¹æ®Šæ–‡å­—ã‚’æœ€å°é™ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
        # \, {, } ã¯æ•°å¼ã‚„ã‚³ãƒãƒ³ãƒ‰ã§ä½¿ã‚ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ãªã„
        escape_map = {
            '&': r'\\&',
            '%': r'\\%',
            '#': r'\\#',
            '_': r'\\_',
        }
        
        for old, new in escape_map.items():
            text = text.replace(old, new)
        
        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã®ãƒœãƒ¼ãƒ«ãƒ‰ã¨ã‚¤ã‚¿ãƒªãƒƒã‚¯
        text = re.sub(r'\*\*(.*?)\*\*', r'\\textbf{\1}', text)
        text = re.sub(r'\*(.*?)\*', r'\\textit{\1}', text)

        # æ”¹è¡Œã‚’é©åˆ‡ã«å‡¦ç†
        text = text.replace('\n', '\\\\ \n')

        # æ•°å¼ãƒ–ãƒ­ãƒƒã‚¯ã‚’å¾©å…ƒ
        for i, math in enumerate(math_blocks):
            text = text.replace(f"PLACEHOLDERMATHBLOCK{i}", math)
            
        return text
        
    except Exception as e:
        st.warning(f"PDFç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ã€æ¥µåŠ›å®‰å…¨ãªãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
        return str(text).replace('&', '').replace('%', '').replace('#', '').replace('_', '')

def generate_response_pdf_safe(question, answer, latex_code=""):
    """å®‰å…¨ãªè³ªå•ã¨å›ç­”ã®PDFç”Ÿæˆï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ï¼‰"""
    try:
        return generate_response_pdf(question, answer, latex_code)
    except Exception as e:
        st.error(f"PDFç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

def build_chat_context(chat_messages, latex_code):
    """ãƒãƒ£ãƒƒãƒˆç”¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰"""
    context = f"å‚è€ƒè³‡æ–™:\n{latex_code}\n\n"
    
    if len(chat_messages) > 1:
        context += "ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´:\n"
        # æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é™¤ã„ãŸæœ€è¿‘ã®ä¼šè©±ã‚’å–å¾—
        recent_messages = chat_messages[-6:]  # æœ€å¤§6ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆ3çµ„ï¼‰
        
        for i in range(0, len(recent_messages) - 1, 2):
            if i + 1 < len(recent_messages):
                user_msg = recent_messages[i]["content"]
                ai_msg = recent_messages[i + 1]["content"]
                context += f"è³ªå•{i//2+1}: {user_msg}\nå›ç­”{i//2+1}: {ai_msg}\n\n"
    
    # æœ€æ–°ã®è³ªå•ã‚’è¿½åŠ 
    if chat_messages:
        latest_question = chat_messages[-1]["content"]
        context += f"æ–°ã—ã„è³ªå•: {latest_question}"
    
    return context

def generate_response_pdf(question, answer, latex_code=""):
    """è³ªå•ã¨å›ç­”ã‚’PDFã¨ã—ã¦å‡ºåŠ›"""
    try:
        # å®‰å…¨ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å‡¦ç†
        def safe_encode(text):
            if isinstance(text, bytes):
                text = text.decode('utf-8', errors='ignore')
            return str(text).encode('utf-8', errors='ignore').decode('utf-8')
        
        # æ–‡å­—åˆ—ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰
        clean_question = safe_encode(question)
        clean_answer = clean_latex_for_pdf(safe_encode(answer))  # AIå›ç­”ã¯ç‰¹æ®Šæ–‡å­—å‡¦ç†ãŒå¿…è¦
        clean_latex_code = safe_encode(latex_code) if latex_code else "ï¼ˆå‚ç…§å†…å®¹ãªã—ï¼‰"
        
        # LaTeX ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆuplatex + æ—¥æœ¬èªå¯¾å¿œï¼‰
        latex_document = """
\\documentclass[12pt,a4paper,uplatex]{jsarticle}
\\usepackage{amsmath}
\\usepackage{amsfonts}
\\usepackage{amssymb}
\\usepackage{geometry}
\\geometry{margin=2.5cm}

\\title{\\textbf{å›ç­”}}
\\author{\\textsc{rigakuGPT}}
\\date{\\today}

\\begin{document}

\\maketitle

\\section{è³ªå•}
\\begin{quote}
""" + clean_question + """
\\end{quote}

\\section{å‚è€ƒè³‡æ–™}
""" + clean_latex_code + """

\\section{å›ç­”}
""" + clean_answer + """

\\vfill
\\begin{center}
\\textbf{rigakuGPT}
\\end{center}

\\end{document}
"""
        
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ PDF ç”Ÿæˆ
        with tempfile.TemporaryDirectory() as tmpdir:
            tex_path = os.path.join(tmpdir, "response.tex")
            
            # UTF-8ã§å®‰å…¨ã«æ›¸ãè¾¼ã¿
            try:
                with open(tex_path, 'w', encoding='utf-8', errors='ignore') as f:
                    f.write(latex_document)
            except UnicodeEncodeError:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ASCIIæ–‡å­—ã®ã¿ã§æ›¸ãè¾¼ã¿
                ascii_document = latex_document.encode('ascii', errors='ignore').decode('ascii')
                with open(tex_path, 'w', encoding='ascii') as f:
                    f.write(ascii_document)
            
            # uplatex + dvipdfmxã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼ˆæ‰‹å‹•å®Ÿè¡Œï¼‰
            try:
                # Step 1: uplatex ã§ .dvi ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
                result1 = subprocess.run([
                    'uplatex', 
                    '-interaction=nonstopmode',
                    '-halt-on-error',
                    'response.tex'
                ], cwd=tmpdir, capture_output=True, text=True, encoding='utf-8', errors='ignore')
                
                if result1.returncode != 0:
                    st.error("uplatex ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼:")
                    st.code(f"stdout: {result1.stdout}")
                    st.code(f"stderr: {result1.stderr}")
                    return None
                
                # Step 2: dvipdfmx ã§ .pdf ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
                result2 = subprocess.run([
                    'dvipdfmx', 
                    'response.dvi'
                ], cwd=tmpdir, capture_output=True, text=True, encoding='utf-8', errors='ignore')
                
                if result2.returncode != 0:
                    st.error("dvipdfmx ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼:")
                    st.code(f"stdout: {result2.stdout}")
                    st.code(f"stderr: {result2.stderr}")
                    return None
                
                pdf_path = os.path.join(tmpdir, "response.pdf")
                
                if os.path.exists(pdf_path):
                    # PDF ã‚’ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼
                    output_dir = "outputs"
                    os.makedirs(output_dir, exist_ok=True)
                    final_pdf_path = os.path.join(output_dir, "rigakuGPT_response.pdf")
                    shutil.copy2(pdf_path, final_pdf_path)
                    return final_pdf_path
                else:
                    st.error("PDF ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                    return None
                    
            except FileNotFoundError as e:
                st.error(f"LaTeX ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {str(e)}")
                st.info("uplatex ã¨ dvipdfmx ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")
                return None
                
    except Exception as e:
        st.error(f"PDF ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

if __name__ == "__main__":
    main()